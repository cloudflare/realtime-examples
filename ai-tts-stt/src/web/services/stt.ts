/**
 * STT (Speech-to-Text) service
 */

import { ApiClient } from './api';
import { WebRTCService } from './webrtc';
import { setState, getState, log } from '../state';
import { TranscriptionMessage } from '../types';

export class STTService {
	private api: ApiClient;
	private webrtc: WebRTCService;
	private transcriptionWS: WebSocket | null = null;
	private transcriptionCloseTimer: number | null = null;

	constructor(sessionId: string) {
		this.api = new ApiClient(sessionId);
		this.webrtc = new WebRTCService();
	}

	async startRecording() {
		const state = getState();
		if (state.sttState.isMicActive) return;

		try {
			log('üé§ Starting microphone (WebRTC publish)...');

			// Get microphone access
			const stream = await WebRTCService.getMicrophoneStream();

			// Create WebRTC connection for audio upload
			const peerConnection = await this.webrtc.createMicConnection(stream);

			// Monitor connection state
			this.webrtc.onConnectionStateChange((pcState) => {
				const connected = pcState === 'connected';
				setState({
					sttState: {
						...getState().sttState,
						pcConnected: connected,
					},
				});
				log(`üì° PeerConnection state: ${pcState}`);
			});

			// Create offer and send to STT adapter
			const offer = await this.webrtc.createOffer();
			const answer = await this.api.sttConnect(offer);
			await this.webrtc.setRemoteAnswer(answer);

			setState({
				sttState: {
					...getState().sttState,
					isMicActive: true,
					startTime: null,
				},
			});

			log('‚úÖ Microphone publishing started. You can start forwarding once connected.');
		} catch (error) {
			log(`‚ùå STT Error: ${(error as Error).message}`);
			this.stopRecording();
			throw error;
		}
	}

	async startForwarding() {
		const state = getState();

		if (!state.sttState.isMicActive) {
			log('‚ö†Ô∏è Start mic first.');
			return;
		}

		if (!state.sttState.pcConnected) {
			log('‚è≥ Waiting for PeerConnection to be connected...');
			return;
		}

		if (state.sttState.isForwarding) {
			log('‚ÑπÔ∏è Forwarding already active.');
			return;
		}

		try {
			log('üîÑ Starting WebSocket forwarding...');
			await this.api.sttStartForwarding();

			setState({
				sttState: {
					...getState().sttState,
					isForwarding: true,
					startTime: Date.now(),
				},
			});

			// Connect to transcription stream if not already connected
			if (!this.transcriptionWS || this.transcriptionWS.readyState === WebSocket.CLOSED) {
				this.connectTranscriptionStream();
			}

			log('‚úÖ WebSocket forwarding started');
		} catch (error) {
			log(`‚ùå Forwarding Error: ${(error as Error).message}`);
			throw error;
		}
	}

	async stopForwarding() {
		const state = getState();

		if (!state.sttState.isForwarding) {
			log('‚ÑπÔ∏è Forwarding already stopped.');
			return;
		}

		try {
			log('‚õî Stopping WebSocket forwarding...');
			await this.api.sttStopForwarding();

			setState({
				sttState: {
					...getState().sttState,
					isForwarding: false,
				},
			});

			// Keep transcription WS connected
			log('‚úÖ Forwarding stopped. Transcription stream remains connected.');
		} catch (error) {
			log(`‚ùå Stop Forwarding Error: ${(error as Error).message}`);
			throw error;
		}
	}

	async stopRecording() {
		const state = getState();

		if (!state.sttState.isMicActive) return;

		if (state.sttState.isForwarding) {
			log('‚ö†Ô∏è Stop forwarding before stopping the mic.');
			return;
		}

		log('‚èπÔ∏è Stopping microphone/WebRTC...');

		this.webrtc.closePeerConnection();

		setState({
			sttState: {
				isMicActive: false,
				isForwarding: false,
				pcConnected: false,
				startTime: null,
			},
		});

		log('‚úÖ Microphone stopped');
	}

	async restartNova() {
		try {
			log('üõ†Ô∏è Restarting Nova STT (debug)...');
			await this.api.sttReconnectNova();
			log('‚úÖ Nova STT restarted');
		} catch (error) {
			log(`‚ùå Restart Nova error: ${(error as Error).message}`);
			throw error;
		}
	}

	private connectTranscriptionStream() {
		const wsUrl = this.api.getTranscriptionStreamUrl();

		this.transcriptionWS = new WebSocket(wsUrl);

		this.transcriptionWS.onopen = () => {
			log('üü¢ Transcription stream connected');
		};

		this.transcriptionWS.onmessage = (event) => {
			try {
				const data: TranscriptionMessage = JSON.parse(event.data);

				if (data.type === 'stt_done') {
					log('‚úÖ Transcription segment finalized');
					return;
				}

				// Handle transcription data
				if (!data.type || data.type === 'transcription') {
					this.displayTranscription(data);
				}
			} catch (error) {
				log(`‚ùå Transcription parse error: ${(error as Error).message}`);
			}
		};

		this.transcriptionWS.onclose = () => {
			log('üîå Transcription stream disconnected');
		};

		this.transcriptionWS.onerror = (error) => {
			log(`‚ùå Transcription WebSocket error: ${error}`);
		};
	}

	private displayTranscription(data: TranscriptionMessage) {
		if (!data.data?.channel?.alternatives?.[0]?.transcript) {
			return;
		}

		const transcript = data.data.channel.alternatives[0].transcript;
		const isFinal = data.data.is_final || false;
		const timestamp = data.timestamp || Date.now();
		const state = getState();

		// Add transcript to state
		const relativeTime = state.sttState.startTime ? (timestamp - state.sttState.startTime) / 1000 : 0;

		setState({
			transcripts: [
				...state.transcripts,
				{
					start: Math.max(0, relativeTime),
					text: transcript,
					timestamp,
					isFinal,
				},
			],
		});
	}

	closeTranscriptionStream() {
		if (this.transcriptionCloseTimer) {
			clearTimeout(this.transcriptionCloseTimer);
			this.transcriptionCloseTimer = null;
		}

		if (this.transcriptionWS) {
			this.transcriptionWS.close();
			this.transcriptionWS = null;
		}
	}

	clearTranscriptions() {
		setState({ transcripts: [] });
		log('üóëÔ∏è Transcriptions cleared');
	}
}
