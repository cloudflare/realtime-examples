# Cloudflare Realtime Audio: TTS + STT

This project demonstrates a real-time text-to-speech (TTS) and speech-to-text (STT) audio streaming app built with Cloudflare Workers, Durable Objects, Cloudflare AI, and the Cloudflare Realtime SFU.

Audio is generated by **Cloudflare AI's `@cf/deepgram/aura-1` model** and streamed to listeners. The demo also includes optional speech-to-text (STT) using **Cloudflare AI's `@cf/deepgram/nova-3` model**.

After deployment, you can interact with the demo entirely from your browser. It provides two interfaces: a **Publisher Console** for controlling the session and a **Listener Page** for passive listening.

## Quickstart (local)

1. npm install
2. Configure `wrangler.jsonc` vars and add secrets (see Configuration below).
3. In two terminals:
   - npm run watch:web
   - npm run dev
4. Open:
   - `http://localhost:8787/<session-name>/publisher`
   - `http://localhost:8787/<session-name>/player`

## How It Works

This project separates audio processing from broadcasting. The `TTSAdapter` and `STTAdapter` handle all audio processing (generation, format conversion, transcription) over WebSockets, while the **Cloudflare Realtime SFU** is responsible for broadcasting all audio to and from clients via WebRTC.

*   **Cloudflare Worker**: The entry point and secure backend. It serves the web UI, handles API requests, and proxies calls to the Cloudflare Realtime SFU, keeping all secrets on the server.
*   **Cloudflare Realtime SFU**: The central hub for **broadcasting** all audio. It ingests streams from the adapters and distributes them globally to listeners over WebRTC.
*   **`TTSAdapter` (Durable Object)**: Manages the text-to-speech flow. [See docs](./TTSAdapter.md).
    - A user sends text to the Worker, which forwards it to the `TTSAdapter`.
    - The adapter sends the text to the Aura model over a persistent WebSocket.
    - Aura streams audio back to the adapter, which processes it (24kHz mono → 48kHz stereo).
    - The adapter sends the processed audio to the SFU over a WebSocket for broadcasting.
    - Listeners connect to the SFU via WebRTC to hear the audio.
*   **`STTAdapter` (Durable Object)**: Manages the speech-to-text flow. [See docs](./STTAdapter.md).
    - A user connects their microphone to the SFU via WebRTC.
    - The SFU forwards the user's audio to the `STTAdapter` over a WebSocket.
    - The adapter processes the audio (48kHz stereo → 16kHz mono) and sends it to the Nova model.
    - Nova returns transcriptions to the adapter.
    - The adapter broadcasts the transcriptions to all connected clients over a WebSocket.
*   **Frontend UI**: A modular TypeScript app in `src/web/` that talks to the Worker. It's bundled into `public/assets/` and served statically.

## Getting Started

### Prerequisites

*   A Cloudflare account (Workers and Durable Objects must be enabled).
*   A configured Cloudflare Realtime SFU application.
*   A Cloudflare API Token with AI access.
*   [Node.js](https://nodejs.org/) and npm.
*   The [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/).

### Configuration

1.  **Clone and Install:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    npm install
    ```

2.  **Set up `wrangler.jsonc`:**
    Open the file and add your Cloudflare account and SFU app IDs to the `vars` block.

    ```jsonc
    {
    	// ... other wrangler config ...
    	"vars": {
            "SFU_API_BASE": "https://rtc.live.cloudflare.com/v1",
    		"CF_ACCOUNT": "<your-cloudflare-account-id>",
    		"REALTIME_SFU_APP_ID": "<your-realtime-sfu-app-id>"
    	}
    }
    ```

3.  **Add Your Secrets:**
    Use Wrangler to store your API keys securely. They will be encrypted and inaccessible to client-side code.

    *   **Cloudflare AI Token:**
        ```bash
        npx wrangler secret put CF_API_TOKEN
        ```

     *   **Cloudflare SFU Token:**
        ```bash
        npx wrangler secret put REALTIME_SFU_BEARER_TOKEN
        ```

### Advanced: SpeexDSP WASM resampler
<details><summary>Build instructions</summary>

For higher-quality audio resampling in Workers, you can build the SpeexDSP library into a WASM module.

Option A (npm):

```bash
npm run tooling:bootstrap
npm run wasm:build
```

Option B (manual):

```bash
# Bootstrap local toolchains (wasi-sdk + Binaryen)
./scripts/bootstrap.sh
# Optional: add to PATH for this shell session
export PATH="$PWD/.tooling/wasi-sdk/bin:$PATH"
export PATH="$PWD/.tooling/binaryen/bin:$PATH"

# Build SpeexDSP and output to src/wasm/
./scripts/build-speexdsp-wasm.sh
```

</details>

### Frontend Development

The frontend is a modular TypeScript app that needs to be bundled before you can run it.

*   **Develop locally:**
    Run these two commands in separate terminal windows.
    ```bash
    # Watch for file changes and rebuild the bundle
    npm run watch:web

    # Start the local development server
    npm run dev
    ```

> URL patterns:
> - Local: `http://localhost:8787/<session-name>/(publisher|player)`
> - Deployed: `https://<worker-name>.<subdomain>.workers.dev/<session-name>/(publisher|player)`

## Deploy and Use

1.  **Deploy:**
    First, build the frontend assets, then deploy the Worker.
    ```bash
    npm run build:web
    npx wrangler deploy
    ```

2.  **Open the Publisher Console:**
    Navigate to your deployed worker's URL using the pattern above. Choose any unique session name you want.

3.  **Start Streaming:**
    Click **Publish Session** to make the audio stream available.

4.  **Share the Listener Link:**
    Give the `/player` URL to anyone you want to share the stream with (see URL patterns above).

5.  **Generate Speech:**
    As the publisher, type text and click **Generate Speech**. The audio will be streamed in real-time to all connected listeners.

6.  **Use Speech-to-Text (Optional):**
    The STT process involves two steps in the UI:
    1.  **Click "Start Mic"**: This calls `POST /<session-name>/stt/connect` to set up the WebRTC connection with the SFU and pre-warm the Nova STT WebSocket.
    2.  **Click "Start Forwarding"**: This button is enabled only after the WebRTC connection state becomes `'connected'`. Clicking it calls `POST /<session-name>/stt/start-forwarding` to begin sending audio from the SFU to the `STTAdapter` for transcription.

    - To stop, click **"Stop Forwarding"** and then **"Stop Mic"**.
    - Transcriptions are streamed back to the client over a separate WebSocket.

7.  **Stop the Session:**
    Click **Unpublish Session** to remove the audio track from Cloudflare's servers and disconnect everyone.

8.  **Debug Cleanup (`DELETE /<session-name>`):**
    For abandoned sessions during demos, you can force a complete cleanup by sending a `DELETE` request to the session root (no `/player` or `/publisher`).
    ```bash
    curl -X DELETE https://<worker-name>.<subdomain>.workers.dev/my-stream
    ```
    Effects:
    - Closes all Durable Object WebSocket clients (SFU subscriber, STT transcription, etc.)
    - Closes upstream AI WebSockets (Aura TTS and Nova STT)
    - Cancels any scheduled alarms
    - Wipes all persisted state for the session (`ctx.storage.deleteAll()`)
    Note: This endpoint is intentionally unauthenticated for this demo. Add authentication before using in production.

## Troubleshooting

*   **Stale UI or 404s on assets?**
    - As in Deploy step, run `npm run build:web`.
    - Check that `wrangler.jsonc` is configured to serve the `public` directory.
    - Try a hard refresh to clear your browser cache.

*   **Buttons not hiding/showing correctly?**
    - Your browser is likely using a cached version of the CSS. Try a hard refresh.

*   **STT gives a "Track not found" error?**
    - See “Deploy and Use → step 6” for the correct Start Mic → wait for 'connected' → Start Forwarding flow.

*   **Microphone not working?**
    - Check your browser's permissions for microphone access.
    - The app requests a 48kHz stereo track with `echoCancellation`, `noiseSuppression`, and `autoGainControl` enabled. Ensure your input device supports these settings.

*   **Subtitle export is empty?**
    - The app only exports finalized transcription segments. Speak for a few seconds and wait for the model to finalize the transcript.

*   **TypeScript errors about DOM types?**
    - The main `tsconfig.json` is for the Worker. For the frontend, use the web-specific config: `npx tsc -p tsconfig.web.json --noEmit`.

*   **Deployed changes not showing up?**
    - See “Deploy and Use → Deploy”.
    - Confirm the Worker is serving `/<session-name>/(publisher|player)` and assets are at `/assets/*`.

## Durable Objects: lifecycle notes

- Outbound AI WebSockets keep the DO “awake.” STT pre-forwarding keeps Nova warm with a 5s KeepAlive; TTS keeps Aura open during/after generation until inactivity closes it.
- Inbound SFU/transcription sockets use the DO Hibernation API, so clients can stay connected while the DO sleeps. Ping/pong is auto-answered (no wake-ups).
- Alarms wake the DO for work (reconnect, inactivity, cleanup). Deadlines are persisted and resumed; the earliest one runs next.
- Buffers are temporary: TTS finalized audio and STT transcription history are in-memory and cleared after a real sleep/restart.
- More details: see [`TTSAdapter.md`](./TTSAdapter.md), [`STTAdapter.md`](./STTAdapter.md), and Cloudflare’s lifecycle guide:
  https://developers.cloudflare.com/durable-objects/concepts/durable-object-lifecycle/